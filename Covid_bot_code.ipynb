{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from newspaper import Article\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt', quiet=True)\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from chatbot import Chat, register_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "model_bert = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 20.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 40.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 60.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 80.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 100.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 120.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 140.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 160.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 180.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 200.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 220.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 240.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 260.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 280.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 300.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 320.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 340.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 360.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 380.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 400.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 420.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 440.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 460.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 480.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 500.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 510.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 530.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 550.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 570.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 590.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 610.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 630.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 650.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 670.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 690.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 710.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 730.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 750.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 770.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 790.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 810.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 830.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 850.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 870.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 890.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 910.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 930.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 940.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 960.00MB\n",
      "INFO:absl:Downloading https://tfhub.dev/google/universal-sentence-encoder/4: 979.63MB\n",
      "INFO:absl:Downloaded https://tfhub.dev/google/universal-sentence-encoder/4, Total size: 987.47MB\n",
      "INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/4'.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://tfhub.dev/google/universal-sentence-encoder/4'\n",
    "model_faq = hub.load(url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_excel(\"C:/Users/susan/MyFolder/BDA/CovidBot/faqs1 (2).xlsx\")\n",
    "state_data = pd.read_csv('C:/Users/susan/MyFolder/BDA/CovidBot/state.csv')\n",
    "country_data = pd.read_csv('C:/Users/susan/MyFolder/BDA/CovidBot/country.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "    num_seg_a = sep_index + 1\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "    start_scores, end_scores = model_bert(torch.tensor([input_ids]),token_type_ids=torch.tensor([segment_ids])) \n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    answer = tokens[answer_start]\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_bert(A):\n",
    "    b = '%20'\n",
    "    Aa = A+' covid-19'\n",
    "    for word in Aa:\n",
    "        if word == ' ':\n",
    "            Aa = Aa.replace(word,b)\n",
    "    url = \"http://news.google.com/news?q=\"+Aa+\"&hl=en-US&sort=date&gl=IN&num=500&output=rss\"\n",
    "    feed_url = url\n",
    "    soup = BeautifulSoup()\n",
    "    text = soup.get_text()\n",
    "    text = text.replace('\\xa0', ' ')\n",
    "    c=''\n",
    "    feeds = feedparser.parse(feed_url).entries\n",
    "    b = feeds[0]['link']\n",
    "    article = Article(b)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    article.nlp()\n",
    "    corpus = article.text\n",
    "    if len(corpus) > 512:\n",
    "        corpus = corpus[:512]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp(corpus):\n",
    "    from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "    from textblob import TextBlob\n",
    "    training_corpus = [\n",
    "                       ('What is the status of covid in kerela?', 'Class_B'),\n",
    "                       (\"Who is the latest celebrith to be tested positive?\", 'Class_B'),\n",
    "                       ('When will vaccines be found?', 'Class_B'),\n",
    "                       ('When will colleges open in karnataka', 'Class_B'),\n",
    "                       ('What is the active case count in Goa?', 'Class_A'),\n",
    "                       ('Total deaths in bangalore', 'Class_A'),\n",
    "                       ('number of tested positive cases in the world', 'Class_A')]\n",
    "\n",
    "    model = NBC(training_corpus) \n",
    "    op = model.classify(corpus)\n",
    "    #print(model.accuracy(test_corpus))\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(input):\n",
    "    return model_faq([input])\n",
    "dataset['Question_Vector'] = dataset.Question.map(embed)\n",
    "dataset['Question_Vector'] = dataset.Question_Vector.map(np.array)\n",
    "questions = dataset.Question\n",
    "QUESTION_VECTORS = np.array(dataset.Question_Vector)\n",
    "COSINE_THRESHOLD = 0.5\n",
    "def cosine_similarity(v1, v2):\n",
    "        mag1 = np.linalg.norm(v1)\n",
    "        mag2 = np.linalg.norm(v2)\n",
    "        if (not mag1) or (not mag2):\n",
    "            return 0\n",
    "        return np.dot(v1, v2) / (mag1 * mag2)\n",
    "def semantic_search(query, data, vectors):        \n",
    "        query_vec = np.array(embed(query))\n",
    "        res = []\n",
    "        for i, d in enumerate(data):\n",
    "            qvec = vectors[i].ravel()\n",
    "            sim = cosine_similarity(query_vec, qvec)\n",
    "            res.append((sim, d[:100], i))\n",
    "        return sorted(res, key=lambda x : x[0], reverse=True)      \n",
    "@register_call(\"news_faqs\")\n",
    "def news_faqs(query,session_id=\"general\"):\n",
    "        '''This will return list of all questions according to their similarity,but we'll pick topmost/most relevant question'''\n",
    "        most_relevant_row = semantic_search(query, questions, QUESTION_VECTORS)[0]\n",
    "        if most_relevant_row[0][0]>=COSINE_THRESHOLD:\n",
    "            answer = dataset.Answer[most_relevant_row[2]]\n",
    "        else:\n",
    "            answer = answer_question(query, news_bert(query))\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_data = pd.read_csv('C:/Users/susan/MyFolder/BDA/CovidBot/state.csv')\n",
    "country_data = pd.read_csv('C:/Users/susan/MyFolder/BDA/CovidBot/country.csv')\n",
    "unimportant=['and']\n",
    "def capitalise(word):\n",
    "    resp=\"\"\n",
    "    v = word.split()\n",
    "    for x in v:\n",
    "        if x not in unimportant:\n",
    "            resp += (\" \" + x.title())\n",
    "        else:\n",
    "            resp += (\" \" + x)\n",
    "\n",
    "    return resp\n",
    "import urllib.request, json\n",
    "with urllib.request.urlopen(\"https://api.covid19india.org/state_district_wise.json\") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    #print(data)\n",
    "#distr = ['Andaman and Nicobar Islands','Andhra Pradesh','Arunachal Pradesh','Assam','Bihar','Chandigarh','Chhattisgarh','Delhi','Dadra and Nagar Haveli and Daman and Diu','Goa','Gujarat','Himachal Pradesh','Haryana','Jharkhand','Jammu and Kashmir','Karnataka','Kerala','Ladakh','Lakshadweep','Maharashtra','Meghalaya','Manipur','Madhya Pradesh','Mizoram','Nagaland','Odisha','Punjab','Puducherry','Rajasthan','Sikkim','Telangana','Tamil Nadu','Tripura','Uttar Pradesh','Uttarakhand','West Bengal']\n",
    "@register_call(\"country_names\")\n",
    "def country_names(query,session_id=\"general\"):\n",
    "    query = capitalise(query).strip()\n",
    "    for i in range(len(country_data)):\n",
    "        if country_data.iloc[i][1]==query:\n",
    "            output = 'TotalCases : '+str(country_data.iloc[i][1])+' NewCases : '+str(country_data.iloc[i][2])+' TotalDeaths : '+str(country_data.iloc[i][3])+' NewDeaths : '+str(country_data.iloc[i][4])+' TotalRecovered : '+str(country_data.iloc[i][5])+' ActiveCases : '+str(country_data.iloc[i][6])\n",
    "            break\n",
    "        else:\n",
    "            output = 'Input valid country name'\n",
    "    return(output)\n",
    "@register_call(\"state_names\")\n",
    "def state_names(query,session_id=\"general\"):\n",
    "    query = capitalise(query).strip()\n",
    "    for i in range(len(state_data)):\n",
    "        if state_data.iloc[i][0]==query:\n",
    "            output = ' Confirmed : '+str(state_data.iloc[i][1])+' Active : '+str(state_data.iloc[i][2])+' Recovered : '+str(state_data.iloc[i][3])+' Deaths : '+str(state_data.iloc[i][4])+' Tested : '+str(state_data.iloc[i][5])\n",
    "            break\n",
    "        else:\n",
    "            output = 'Input valid country name'\n",
    "    return(output)\n",
    "@register_call(\"district_names\")\n",
    "def district_names(query,session_id=\"general\"):\n",
    "    query = capitalise(query).strip()\n",
    "    for i in data:\n",
    "        for j in data[i]['districtData']:\n",
    "            if j == query:\n",
    "                output =  ' Active Cases : '+str(data[i]['districtData'][j]['active'])+' Confirmed : '+str(data[i]['districtData'][j]['confirmed'])+' Dead : '+str(data[i]['districtData'][j]['deceased'])+' Recovered : '+str(data[i]['districtData'][j]['recovered'])\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The death rate is currently decreasing.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstQuestion=\"what is death rate in india?\"\n",
    "if nlp(firstQuestion) == 'Class_B':\n",
    "    temp = \"news.template\"\n",
    "else:\n",
    "    temp = 'demo.template'\n",
    "Chat(temp).say(firstQuestion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "firstQuestion=\"count in Haveri\"\n",
    "Chat(temp).say(firstQuestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import urllib.request, json\n",
    "with urllib.request.urlopen(\"https://api.covid19india.org/state_district_wise.json\") as url:\n",
    "    data = json.loads(url.read().decode())\n",
    "    #print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distr = ['Andaman and Nicobar Islands','Andhra Pradesh','Arunachal Pradesh','Assam','Bihar','Chandigarh','Chhattisgarh','Delhi','Dadra and Nagar Haveli and Daman and Diu','Goa','Gujarat','Himachal Pradesh','Haryana','Jharkhand','Jammu and Kashmir','Karnataka','Kerala','Ladakh','Lakshadweep','Maharashtra','Meghalaya','Manipur','Madhya Pradesh','Mizoram','Nagaland','Odisha','Punjab','Puducherry','Rajasthan','Sikkim','Telangana','Tamil Nadu','Tripura','Uttar Pradesh','Uttarakhand','West Bengal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unimportant=['and']\n",
    "def capitalise(word):\n",
    "    resp=\"\"\n",
    "    v = word.split()\n",
    "    for x in v:\n",
    "        if x not in unimportant:\n",
    "            resp += (\" \" + x.title())\n",
    "        else:\n",
    "            resp += (\" \" + x)\n",
    "    return resp\n",
    "def state_names(question):\n",
    "    question = capitalise(question).strip()\n",
    "    for i in range(0,len(state_data)):\n",
    "        for j in question.split(' '):\n",
    "            if capitalise(j)[1:] == state_data.iloc[i]['Location']:\n",
    "                output = 'state name : '+ (state_data.iloc[i]['Location'])+'; Active Cases : '+str(state_data.iloc[i][1])+'; New Cases : '+str(state_data.iloc[i][2])+'; Cases per 1 million people : '+str(state_data.iloc[i][4])+'; Deaths : '+str(state_data.iloc[i][5])\n",
    "                \n",
    "    else:\n",
    "        for i in distr:\n",
    "            #print(data[i]['districtData'])\n",
    "            for j in data[i]['districtData']:\n",
    "                for k in question.split(' '):\n",
    "                    if k == j:\n",
    "                        output = ' Active Cases : '+str(data[i]['districtData'][j]['active'])+' Confirmed : '+str(data[i]['districtData'][j]['confirmed'])+' Dead : '+str(data[i]['districtData'][j]['deceased'])+' Recovered : '+str(data[i]['districtData'][j]['recovered'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def dist_names(question):\n",
    "    question = capitalise(question).strip()\n",
    "    for i in distr:\n",
    "        #print(data[i]['districtData'])\n",
    "        for j in data[i]['districtData']:\n",
    "            for k in question.split(' '):\n",
    "                if k == j:\n",
    "                    output = ' Active Cases : '+str(data[i]['districtData'][j]['active'])+' Confirmed : '+str(data[i]['districtData'][j]['confirmed'])+' Dead : '+str(data[i]['districtData'][j]['deceased'])+' Recovered : '+str(data[i]['districtData'][j]['recovered'])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import urllib.request, json\n",
    "with urllib.request.urlopen(\"https://api.covid19india.org/data.json\") as url:\n",
    "    data_dist = json.loads(url.read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in data_dist['statewise']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def state_names(query):\n",
    "    query = capitalise(query).strip()\n",
    "    for i in data_dist['statewise']:\n",
    "        if i['state']==query:\n",
    "            output = ' Confirmed : '+str(i['confirmed'])+' Active : '+str(i['active'])+' Recovered : '+str(i['recovered'])+' Deaths : '+str(i['deaths'])\n",
    "            break\n",
    "        else:\n",
    "            output = 'Input valid country name'\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import urllib.request, json\n",
    "with urllib.request.urlopen(\"https://api.covid19api.com/summary\") as url:\n",
    "    data_country = json.loads(url.read().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in data_country['Countries']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def country_names(query):\n",
    "    query = capitalise(query).strip()\n",
    "    for i in data_country['Countries']:\n",
    "        if i['Country']==query:\n",
    "            output = 'TotalCases : '+str(i['TotalConfirmed'])+' NewCases : '+str(i['NewConfirmed'])+' TotalDeaths : '+str(i['TotalDeaths'])+' NewDeaths : '+str(i['NewDeaths'])+' TotalRecovered : '+str(i['TotalRecovered'])\n",
    "            break\n",
    "        else:\n",
    "            output = 'Input valid country name'\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "country_names('Armenia')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
